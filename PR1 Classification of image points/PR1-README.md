# Подробное, шаг-за-шаговое объяснение кода + теория (максимально подробно)

Отлично — разберём каждую часть вашего скрипта подробно: что она делает, почему так, математика за ней и возможные улучшения/ошибки. Буду идти по хронологии файла и вставлять теорию там, где она нужна.

---

## 1) Импорты и список файлов

```python
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

image_paths = [
    'data/2026-02-13-00_00_2026-02-13-23_59_Sentinel-2_L2A_NDWI.jpg',
    'data/2026-02-13-00_00_2026-02-13-23_59_Sentinel-2_L2A_SWIR.jpg',
    'data/2026-02-13-00_00_2026-02-13-23_59_Sentinel-2_L2A_NDVI.jpg',
    'data/2026-02-13-00_00_2026-02-13-23_59_Sentinel-2_L2A_Highlight_Optimized_Natural_Color-2.jpg'
]
```

**Что делает:**
Импортирует библиотеки: `numpy` для чисел/матриц, `matplotlib` для сохранения/визуализации, `PIL.Image` для чтения изображений. `image_paths` — список 4 изображений (каждое — JPEG), их имена предполагают, что это спутниковые продукты (NDWI, SWIR, NDVI и натуральный цвет).

**Замечание по данным:**
Судя по именам, у вас 4 разных визуализации/индекса изображения одной сцены. Вместо чтения только отдельных каналов Sentinel (B2/B3/...), вы читаете уже сохранённые RGB-JPEG версии этих продуктов. Следствие: значения — цифровые уровни (0–255), не отражают напрямую физические рефлектансы, если JPEG/визуализация была масштабирована/настроена.

---

## 2) Формирование массива каналов (feature stack)

```python
channels = []
for path in image_paths:
    img = Image.open(path).convert('RGB')
    img_array = np.array(img, dtype=np.float32)
    red = img_array[:, :, 0]
    green = img_array[:, :, 1]
    blue = img_array[:, :, 2]
    
    channels.append(red)
    channels.append(green)
    channels.append(blue)

channels = np.array(channels)
height, width = channels.shape[1], channels.shape[2]
```

**Что делает:**

* Для каждого из 4 изображений:

  * открывает файл и приводит к формату RGB (`.convert('RGB')` гарантирует 3 канала);
  * переводит в `numpy` массив `float32`;
  * извлекает три канала (R, G, B) как отдельные 2D-матрицы;
  * добавляет их в список `channels`.
* Затем `channels` преобразуется в numpy-массив формы `(num_channels, height, width)`.

**Итог:**

* `num_channels = 4 images * 3 channels = 12`.
* `channels.shape = (12, height, width)`.

**Замечания / потенциальные проблемы:**

* Вы используете `float32` **но не нормируете** (не переводите в [0,1] или в физ.рефлектанс). Это допустимо, но тогда параметры классов (средние, ковариации) будут в шкале 0–255. При сравнении между классами это нормально, но если изображения были визуально контрастированы/клипованы/кодированы JPEG — это может внести искажения.
* Порядок осей: `channels[:, y, x]` — правильный способ обратиться к пикселю (первый индекс — канал).
* Память: размер в байтах ≈ `12 * height * width * 4 bytes`. Для больших кадров это может быстро расти.

---

## 3) ROIs — выбор эталонных регионов (ручные примеры)

Вы объявили словари с координатами прямоугольников:

```python
water = [ (1041, 243, 1045, 247), ... ]
words = [ ... ]
urban = [ ... ]
vegetation = [ ... ]
trees = [ ... ]
all_interests = {
    'Вода': water,
    'Слова': words,
    'Застройка': urban,
    'Растительность': vegetation,
    'Деревья': trees
}
```

**Как интерпретируются координаты:**

* Каждая запись — кортеж `(x1, y1, x2, y2)`.
* При извлечении вы делаете `channels[:, y1:y2, x1:x2]` — то есть `y` соответствует строкам (высота), `x` — столбцам (ширина). Это правильный порядок для NumPy (row, col = y, x).
* Важный момент: в срезах `y1:y2` и `x1:x2` значение `y2` и `x2` **исключается** из результата (так работает Python). То есть фактическая ширина прямоугольника = `x2-x1`, высота = `y2-y1`.

**Проверки:**

* Если в одном кортеже `x1 == x2` или `y1 == y2`, ROI будет пустым. В коде этого не проверяется — стоит добавить assert/проверку размеров.
* ROI малых размеров (несколько пикселей) даст мало статистики, ковариация может стать вырожденной.

---

## 4) Цвета классов и имена

```python
class_colors = {
    'Вода': (0, 0, 255),
    'Слова': (0, 0, 0),
    'Застройка': (255, 255, 255),
    'Растительность': (0, 128, 0),
    'Деревья': (1, 50, 32)
}
class_names = ['Вода', 'Слова', 'Застройка', 'Растительность', 'Деревья']
```

**Что стоит отметить:**

* Цвета заданы в 0–255 RGB, далее вы делите на 255 при визуализации (`color = np.array(class_colors[class_name]) / 255.0`) — корректно.
* `Деревья` имеет цвет `(1,50,32)` — не ошибка, просто очень тёмный/ближний к зелёному.

---

## 5) Подготовка параметров классов (оценка среднего и ковариации)

Основной фрагмент:

```python
for class_name in class_names:
    all_pixels = []
    
    for interest in all_interests[class_name]:
        x1, y1, x2, y2 = interest
        x1, x2 = int(min(x1, x2)), int(max(x1, x2))
        y1, y2 = int(min(y1, y2)), int(max(y1, y2))
        
        roi_pixels = channels[:, y1:y2, x1:x2]  # (num_channels, h, w)
        roi_pixels = roi_pixels.reshape(len(channels), -1).T  # (num_pixels, num_channels)
        all_pixels.append(roi_pixels)
    
    all_pixels = np.vstack(all_pixels)
    
    mean_vector = np.mean(all_pixels, axis=0)
    
    centered = all_pixels - mean_vector
    N = all_pixels.shape[0]
    cov_matrix = (centered.T @ centered) / (N - 1)
    
    class_params[class_name] = { 'mean': mean_vector, 'cov': cov_matrix, 'num_pixels': len(all_pixels) }
```

### Теория: оценка среднего и ковариации

* Пусть для класса у вас есть `N` векторов признаков ( x_i \in \mathbb{R}^d ) (здесь ( d = 12 ) каналов).
  **Выборочное среднее**:
  [
  \hat{\mu} = \frac{1}{N} \sum_{i=1}^N x_i
  ]
* **Матрица выборочной ковариации (не смещённая)**:
  [
  \hat{\Sigma} = \frac{1}{N-1} \sum_{i=1}^N (x_i - \hat{\mu})(x_i - \hat{\mu})^\top
  ]
  В коде это реализовано как `(centered.T @ centered) / (N-1)`.
* `N-1` используется (не `N`) чтобы получить несмещённую оценку ковариации.

### Практические замечания:

* Если (N) мало (особенно (N \le d)), (\hat{\Sigma}) может быть вырожденной (сингулярной) — нельзя вычислить обратную напрямую.
* Вы добавите регуляризацию позже (E = identity) при вычислении расстояния, потому что ковариации могут не быть положительно-определёнными.

---

## 6) Классификация пикселей — основной цикл

```python
classification_map = np.zeros((height, width), dtype=np.int32)

total_pixels = height * width
processed = 0

for y in range(height):
    for x in range(width):
        pixel = channels[:, y, x]
        
        min_distance = float('inf')
        assigned_class = 0
        
        for class_id, class_name in enumerate(class_names):
            mean = class_params[class_name]['mean']
            cov = class_params[class_name]['cov']
            
            E = np.eye(cov.shape[0])
            S_plus_E = cov + E
            
            try:
                inv_matrix = np.linalg.inv(S_plus_E)
            except np.linalg.LinAlgError:
                inv_matrix = np.linalg.pinv(S_plus_E)
            
            diff = pixel - mean
            distance = np.sqrt(diff.T @ inv_matrix @ diff)
            
            if distance < min_distance:
                min_distance = distance
                assigned_class = class_id
        
        classification_map[y, x] = assigned_class
        
        processed += 1
        if processed % 10000 == 0:
            progress = (processed / total_pixels) * 100
            print(f"   Обработано: {progress:.1f}%")
```

### Теория: евклидово-махаланобисово расстояние

* **Махаланобисово расстояние** между вектором (x) и распределением с (\mu, \Sigma):
  [
  d_M(x, \mu) = \sqrt{(x-\mu)^\top \Sigma^{-1} (x-\mu)}
  ]
* Что делает код: для каждого класса берёт ковариацию (\Sigma), добавляет единичную матрицу (E) (регуляризация), вычисляет её обратную матрицу и считает квадратный корень квадратичной формы — это и есть Mahalanobis distance.
* Вы присваиваете пиксель тому классу, для которого это расстояние минимально.

### Почему добавляют `E = I` (регуляризация)?

* Если (\Sigma) сингулярна (нет обратной), `np.linalg.inv` выдаст ошибку. Добавление (I) (или (\lambda I)) повышает собственные значения на (\lambda), делая матрицу положительно-определённой и обратимой. Это называется **ридж-регуляризация** или **Tikhonov regularization**.
* В коде добавляется именно `1 * I`. Часто используют маленькое число (\lambda) (например `1e-3`, `1e-6`) вместо `1`, чтобы не искажать слишком сильно матрицу ковариации.

### Замечания по реализации (производительность и корректность):

1. **Вычисление обратной матрицы внутри цикла по каждому пикселю** — очень дорого!
   Поскольку `cov` не меняется для одного класса, обратную матрицу `inv_matrix` нужно **предвычислить один раз для каждого класса** (до цикла по пикселям).
2. `np.linalg.inv` + `np.sqrt` на каждую итерацию — накладно; лучше:

   * Предвычислить `inv_matrix` для каждого класса.
   * Для множества пикселей использовать векторизацию (см. раздел "Оптимизации" ниже).
3. Вы отбрасываете лог-нормирующий член квадратичной формы, детерминант и приоритеты классов, — вы по сути используете **критерий ближайшего по Mahalanobis**, что эквивалентно максимизации лог-вероятности при Gaussian с общей (или нормированной) ковариацией при равных априорных вероятностях *и* игнорировании детерминанта ковариации. Для полного квадратичного дискриминатора (QDA) нужно учитывать также (\log |\Sigma|) и априорные вероятности (P(C)).

---

## 7) Визуализация результата

```python
colored_map = np.zeros((height, width, 3))
for class_id, class_name in enumerate(class_names):
    color = np.array(class_colors[class_name]) / 255.0
    mask = classification_map == class_id
    colored_map[mask] = color

plt.figure(figsize=(16, 10))
plt.imshow(colored_map)
plt.axis('off')
plt.tight_layout()
plt.savefig('results/classification_result.png', dpi=300, bbox_inches='tight')
```

**Что делает:**
Создаёт RGB-изображение `colored_map`, где каждому пикселю присваивается цвет класса (нормализованный в [0,1]). Сохраняет итоговый PNG.

**Замечание:** убедитесь, что папка `results/` существует, иначе `savefig` упадёт.

---

## 8) Полный разбор математических и практических нюансов

### 8.1 Интерпретация Mahalanobis в контексте классификации

* Mahalanobis учитывает не только расстояние по координатам, но и ковариации между признаками. Если два признака сильно коррелируют, расстояние «в направлении корреляции» считается меньше (т.е. направление с большой дисперсией менее информативно).
* Именно поэтому Mahalanobis полезен для многомерных данных с разной шкалой и корреляциями.

### 8.2 Почему нужно или не нужно нормировать каналы

* Если все каналы в одной шкале (0–255) и одинаково важны, нормализация не обязательна.
* Но если некоторые каналы имеют иное динамическое диапазона (например, один индекс NDVI в -1..1, а другой — 0..255), надо привести к общей шкале (z-score, minmax) или использовать физические рефлектансы.

### 8.3 Когда ковариация будет сингулярной

* Когда число эталонных пикселей (N \le d) (12), (\hat{\Sigma}) будет рангово-дефицитной.
* Небольшое (N) делает оценку ковариации шумной — лучше взять больше ROI/пикселей.
* Регуляризация ((\Sigma + \lambda I)) уменьшает проблему.

### 8.4 QDA vs LDA vs Mahalanobis

* **LDA (линейный дискриминант)** предполагает, что все классы имеют одну и ту же ковариацию (\Sigma). Решение — линейная граница; чаще вычисляют общую ковариацию и используют её.
* **QDA (квадратичный дискриминант)** допускает разную (\Sigma_k) для каждого класса. Решение — квадратичная граница. Полный QDA использует ((x-\mu_k)^\top \Sigma_k^{-1} (x-\mu_k) + \log|\Sigma_k| - 2 \log P(C_k)).
* Ваш способ — ближе к использованию QDA без (\log|\Sigma|) и без априорных логарифмов, т.е. сравниваете только квадратичные формы.

---

## 9) Потенциальные баги и улучшения (рекомендации)

### Ошибки/опасности

* **Регуляризация в фиксированном виде `+ I`**: добавлять `1` — возможно слишком грубо. Лучше `lambda * I` с маленькой `lambda` (напр. `1e-3 * np.trace(cov)/d`).
* **Инверсия в каждом пикселе** — большая накладная ошибка.
* **Отсутствие проверки на пустые ROIs** — возможен crash если ROI пустой.
* **Неучёт детерминанта (log |Σ|)** — может смещать выбор класса, особенно если ковариации разных классов имеют сильно отличающиеся детерминанты.
* **JPEG и визуализация** — исходные каналы могли быть уже скорректированы, поэтому статистики будут не физические.

### Производительность — как ускорить

1. **Предвычислить `inv_matrix` для каждого класса** и, если нужно, `logdet = np.log(np.linalg.det(S_plus_E))`.

2. **Векторизация по всем пикселям**:

   * Преобразовать изображение в массив `pixels = channels.reshape(d, -1).T` (форма `(num_pixels, d)`) и вычислить расстояния к всем классам используя `np.einsum` или матричные операции.
   * Пример (упрощённый):

     ```python
     pixels = channels.reshape(d, -1).T  # (P, d)
     invs = [precomputed inverse per class]  # list of (d,d)
     mus = [mean vectors]
     dists = np.empty((P, num_classes))
     for k in range(num_classes):
         diff = pixels - mus[k]  # (P, d)
         # квадратичная форма для каждого пикселя: (diff * (inv @ diff.T)).sum(axis=1)
         tmp = diff @ invs[k]    # (P, d)
         dists[:, k] = np.einsum('ij,ij->i', tmp, diff)  # (P,)
     assigned = dists.argmin(axis=1)
     classification_map = assigned.reshape(height, width)
     ```
   * Это обычно в 10–100× быстрее, т.к. инверсии делаются только `num_classes` раз.

3. **Использовать `scipy.linalg.solve` или `cho_solve`**: безопаснее и быстрее, чем брать `inv` затем умножать. Но если вы заранее используете `inv` только один раз, то `solve(inv, ...)` заменяет прямую инверсию.

4. **Использовать пакет `scikit-learn`**:

   * `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis` или `GaussianNB` (наивный байес) — уже оптимизированы.
   * QDA автоматически справится с регуляризацией (`reg_param`).

### Улучшение регуляризации (пример):

```python
# предпочтительнее: small lambda scaled by trace
lambda_ = 1e-3 * np.trace(cov) / cov.shape[0]
S_plus = cov + lambda_ * np.eye(cov.shape[0])
```

---

## 10) Примеры улучшенного/оптимизированного кода (вставка — как переписать критическую часть)

Ниже — примерный код, где:

* предвычисляем `inv` и `logdet`,
* векторизуем подсчёт расстояний и назначение классов.

```python
# d = number of channels
d = channels.shape[0]
P = height * width
pixels = channels.reshape(d, -1).T  # (P, d)

# Предвычислим инверсии с регуляризацией
invs = []
for class_name in class_names:
    cov = class_params[class_name]['cov']
    # регуляризация
    lambda_ = 1e-3 * np.trace(cov) / cov.shape[0]
    S = cov + lambda_ * np.eye(d)
    invS = np.linalg.inv(S)  # или np.linalg.pinv(S)
    invs.append(invS)
    # также можно сохранить лог-детерминант, если захотите QDA:
    # sign, logdet = np.linalg.slogdet(S)

# Векторизованная матрица расстояний
dists = np.empty((P, len(class_names)), dtype=np.float32)
for k, class_name in enumerate(class_names):
    mu = class_params[class_name]['mean']  # (d,)
    diff = pixels - mu  # (P, d)
    tmp = diff @ invs[k]  # (P, d)
    # квадратичная форма без корня (мониторинг sqrt не обязателен для сравнения)
    dists[:, k] = np.einsum('ij,ij->i', tmp, diff)  # (P,)

assigned = dists.argmin(axis=1).astype(np.int32)
classification_map = assigned.reshape(height, width)
```

**Пояснение:** нет необходимости брать `sqrt`, т.к. (\sqrt{\cdot}) монотонна — минимум сохранится.

---

## 11) Отдельные мелкие советы и улучшения качества

* **Проверка ROI**: перед использованием ставить `if x2<=x1 or y2<=y1: continue` и логировать.
* **Баланс классов**: если у некоторых классов мало пикселей, можно выполнить oversampling ROI или добавить больше ROI.
* **Стандартизация признаков**: `z = (x - mean_all)/std_all` поможет, если один канал имеет гораздо большую дисперсию.
* **Использование физических каналов**: по возможности использовать исходные спектральные каналы Sentinel (B2/B3/B4/B8/... ) и отражения, а не визуализированные JPEG.
* **Оценка результата**: если есть векторные метки/периодическая валидация, посчитать confusion matrix, accuracy, IoU для каждого класса.
* **Отладка цветов**: `class_colors` можно хранить как нормализованные `[0,1]` заранее.

---

## 12) Подытоживание — что делает ваш код в 3 предложениях

1. Читает 4 RGB-изображения одной сцены, формирует вектор признаков для каждого пикселя из 12 каналов (4×R,G,B).
2. Для каждого класса (ручные ROI) оценивает выборочное среднее и ковариацию по этим 12-мерным признакам.
3. Классифицирует каждый пиксель, назначая тот класс, для которого Mahalanobis-расстояние до класса минимально (с добавлением `I` к ковариации для регуляризации), и визуализирует результат.


